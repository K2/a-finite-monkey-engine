#!/usr/bin/env python3
"""
Script to analyze and compare prompt quality generated by different LLMs.
Helps determine which models produce the most effective prompts.
"""
import os
import sys
import json
import argparse
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional
from loguru import logger
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

# Import vector store
from vector_store_util import SimpleVectorStore

async def analyze_prompts(
    vector_store_dir: str,
    collection_name: str,
    output_file: Optional[str] = None,
    reevaluate: bool = False,
    judge_model: Optional[str] = None
) -> Dict[str, Any]:
    """
    Analyze prompts in a vector store collection to determine which LLM generates the best prompts.
    
    Args:
        vector_store_dir: Directory containing the vector store
        collection_name: Name of the collection to analyze
        output_file: Path to save detailed results (optional)
        reevaluate: Whether to reevaluate prompts even if they've been evaluated before
        judge_model: Model to use for judging prompt quality
        
    Returns:
        Analysis results
    """
    logger.info(f"Analyzing prompts in {vector_store_dir}/{collection_name}")
    
    # Initialize vector store
    vector_store = SimpleVectorStore(
        storage_dir=vector_store_dir,
        collection_name=collection_name
    )
    
    # Set judging model if provided
    if judge_model:
        os.environ["PROMPT_JUDGE_MODEL"] = judge_model
    
    # Initialize result counters
    models_count = {}
    models_wins = {}
    models_scores = {}
    documents_with_multi_prompts = 0
    total_documents = len(vector_store._documents)
    
    console = Console()
    console.print(f"[bold]Analyzing {total_documents} documents in {collection_name}[/bold]")
    
    # Analyze documents with multi-LLM prompts
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}"),
        BarColumn(),
        TextColumn("[bold]{task.completed}/{task.total}"),
        TimeElapsedColumn(),
    ) as progress:
        task = progress.add_task("[green]Analyzing prompts", total=total_documents)
        
        for i, doc in enumerate(vector_store._documents):
            progress.update(task, advance=1, description=f"[green]Analyzing document {i+1}/{total_documents}")
            
            metadata = doc.get('metadata', {})
            prompts = metadata.get('prompts', {})
            
            # Skip documents without multi-LLM prompts
            if not prompts or not isinstance(prompts, dict) or 'primary' not in prompts:
                continue
                
            documents_with_multi_prompts += 1
            
            # Count models used
            primary_model = prompts['primary'].get('model', 'unknown')
            models_count[primary_model] = models_count.get(primary_model, 0) + 1
            
            # Process secondary models
            if 'secondary' in prompts and prompts['secondary']:
                for model in prompts['secondary'].keys():
                    models_count[model] = models_count.get(model, 0) + 1
            
            # Check existing evaluation or request reevaluation
            evaluation = prompts.get('evaluation', {})
            if not evaluation or reevaluate:
                if 'secondary' in prompts and prompts['secondary']:
                    # Get primary and secondary prompts
                    primary_prompt = prompts['primary'].get('prompt', '')
                    secondary_prompts = {model: data.get('prompt', '') 
                                        for model, data in prompts['secondary'].items()}
                    
                    # Evaluate prompts
                    evaluation = await vector_store._evaluate_prompts(
                        primary_prompt=primary_prompt,
                        secondary_prompts=secondary_prompts,
                        document=doc
                    )
                    
                    # Update document with evaluation
                    prompts['evaluation'] = evaluation
                    doc['metadata']['prompts'] = prompts
                    
                    # Save updated metadata
                    vector_store._save_document_metadata(os.path.join(vector_store_dir, collection_name))
            
            # Process evaluation results
            if evaluation and 'best_model' in evaluation:
                best_model = evaluation['best_model']
                if best_model:
                    models_wins[best_model] = models_wins.get(best_model, 0) + 1
                
                # Track scores
                if 'heuristic_scores' in evaluation:
                    for model, score in evaluation['heuristic_scores'].items():
                        if model not in models_scores:
                            models_scores[model] = []
                        models_scores[model].append(score)
    
    # Calculate average scores
    avg_scores = {}
    for model, scores in models_scores.items():
        if scores:
            avg_scores[model] = sum(scores) / len(scores)
    
    # Prepare results
    results = {
        "total_documents": total_documents,
        "documents_with_multi_prompts": documents_with_multi_prompts,
        "models_count": models_count,
        "models_wins": models_wins,
        "average_scores": avg_scores
    }
    
    # Display results
    console.print("\n[bold]Prompt Analysis Results[/bold]")
    
    # Models table
    models_table = Table(title="Models Performance")
    models_table.add_column("Model", style="cyan")
    models_table.add_column("Documents", style="green")
    models_table.add_column("Wins", style="yellow")
    models_table.add_column("Win Rate", style="magenta")
    models_table.add_column("Avg Score", style="blue")
    
    for model in sorted(models_count.keys()):
        count = models_count[model]
        wins = models_wins.get(model, 0)
        win_rate = f"{(wins / count * 100):.1f}%" if count > 0 else "N/A"
        avg_score = f"{avg_scores.get(model, 0):.2f}"
        
        models_table.add_row(model, str(count), str(wins), win_rate, avg_score)
    
    console.print(models_table)
    
    # Save detailed results if requested
    if output_file:
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        console.print(f"\nDetailed results saved to [bold]{output_file}[/bold]")
    
    return results

async def main():
    parser = argparse.ArgumentParser(description="Analyze prompt quality across different LLMs")
    
    parser.add_argument(
        "-d", "--directory",
        default=None,
        help="Vector store directory"
    )
    
    parser.add_argument(
        "-c", "--collection",
        required=True,
        help="Collection name to analyze"
    )
    
    parser.add_argument(
        "-o", "--output",
        default=None,
        help="File to save detailed results"
    )
    
    parser.add_argument(
        "--reevaluate",
        action="store_true",
        help="Reevaluate prompts even if they were evaluated before"
    )
    
    parser.add_argument(
        "--judge-model",
        default=None,
        help="Model to use for judging prompt quality"
    )
    
    args = parser.parse_args()
    
    # Initialize logging
    logger.remove()
    logger.add(sys.stderr, level="INFO")
    
    # Determine vector store directory
    vector_store_dir = args.directory
    if not vector_store_dir:
        try:
            from finite_monkey.nodes_config import config
            vector_store_dir = getattr(config, "VECTOR_STORE_DIR", "./vector_store")
        except ImportError:
            vector_store_dir = "./vector_store"
    
    # Run analysis
    await analyze_prompts(
        vector_store_dir=vector_store_dir,
        collection_name=args.collection,
        output_file=args.output,
        reevaluate=args.reevaluate,
        judge_model=args.judge_model
    )

if __name__ == "__main__":
    asyncio.run(main())
