"""
Standalone vector store utility using multiple embedding options,
with configuration derived from the application's settings hierarchy.
"""
import os
import json
import asyncio
import hashlib
import pickle
import re
import functools
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from loguru import logger
from datetime import datetime
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn, TimeRemainingColumn
from rich.console import Console
from rich.table import Table

try:
    # Import config from the same hierarchy used in the main application
    from finite_monkey.nodes_config import config as app_config
except ImportError:
    # Fallback config if running standalone
    logger.warning("Could not import main application config, using defaults")
    app_config = type('DefaultConfig', (), {
        'VECTOR_STORE_DIR': "./vector_store",
        'EMBEDDING_MODEL': "local",
        'EMBEDDING_DEVICE': "auto",
        'IPEX_MODEL': "BAAI/bge-small-en-v1.5",
        'IPEX_FP16': False,
        'OLLAMA_MODEL': "nomic-embed-text",
        'OLLAMA_URL': "http://localhost:11434",
        'GENERATE_PROMPTS': True,
        'USE_OLLAMA_FOR_PROMPTS': True,
        'PROMPT_MODEL': "gemma:2b",
        'MULTI_LLM_PROMPTS': False
    })

def run_sync(func):
    """Run a synchronous function in an asynchronous manner."""
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        loop = asyncio.get_event_loop()
        wrapped = functools.partial(func, *args, **kwargs)
        return await loop.run_in_executor(None, wrapped)
    return wrapper

from llama_index.core.embeddings import BaseEmbedding
from pydantic import BaseModel, Field

class IPEXEmbedding(BaseEmbedding):
    """
    Intel IPEX optimized embedding model for high-performance inference.
    
    This embedding model leverages Intel optimizations for better performance
    on Intel CPUs and XPUs (GPUs).
    """
    
    def __init__(self, model_name: str, device: str = "auto", use_fp16: bool = False):
        """
        Initialize the IPEX embedding model.
        
        Args:
            model_name: HuggingFace model to use for embeddings
            device: Device to run on ("auto", "cpu", or "xpu")
            use_fp16: Whether to use FP16 precision for better performance
        """
        # Initialize base class first
        super().__init__()
        
        # Store settings as regular instance variables, not using model_kwargs
        self._model_name = model_name
        self._device_setting = device
        self._use_fp16 = use_fp16
        self._tokenizer = None
        self._model = None
        self._device_type = "cpu"  # Default, will be updated during initialization
        
        # Initialize model
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize the model with IPEX optimizations."""
        try:
            import torch
            # Disable torchvision ops registration
            os.environ["TORCH_DISABLE_CUSTOM_OPERATIONS"] = "1"
            
            from transformers import AutoTokenizer, AutoModel
            
            # Get settings from instance variables
            device_setting = self._device_setting
            use_fp16 = self._use_fp16
            
            # Define dtype based on fp16 setting
            dtype = torch.float16 if use_fp16 else torch.float32
            
            # Determine device to use
            device_type = "cpu"  # Default to CPU
            ipex_available = False
            
            # Check if IPEX is installed
            try:
                import intel_extension_for_pytorch as ipex
                ipex_available = True
                logger.info("Intel Extension for PyTorch is available")
                
                # Check for XPU if device is auto or xpu
                if device_setting in ["auto", "xpu"] and hasattr(torch, 'xpu'):
                    if torch.xpu.is_available():
                        device_type = "xpu"
                        logger.info(f"Using XPU device: {torch.xpu.get_device_name()}")
                    else:
                        logger.info("XPU device requested but not available, falling back to CPU")
            except ImportError:
                logger.warning("Intel Extension for PyTorch not found, using standard PyTorch")
            
            # Load model and tokenizer
            logger.info(f"Loading embedding model: {self._model_name}")
            
            # Initialize tokenizer
            tokenizer = AutoTokenizer.from_pretrained(self._model_name)
            self._tokenizer = tokenizer
            
            # Initialize model
            model = AutoModel.from_pretrained(self._model_name)
            model = model.eval()  # Set to evaluation mode
            
            # Apply optimizations based on available hardware
            if ipex_available:
                if device_type == "cpu":
                    logger.info("Applying IPEX CPU optimizations")
                    try:
                        # Create proper sample input for IPEX
                        sample_text = "Sample text for optimization"
                        sample_inputs = tokenizer(sample_text, return_tensors="pt")
                        
                        model = ipex.optimize(
                            model,
                            dtype=dtype,
                            auto_kernel_selection=True,
                            sample_input=tuple(sample_inputs.values()),
                            weights_prepack=False
                        )
                        logger.info("IPEX CPU optimizations applied successfully")
                    except RuntimeError as e:
                        logger.warning(f"Error applying IPEX optimizations: {e}")
                elif device_type == "xpu":
                    logger.info("Moving model to XPU and applying optimizations")
                    try:
                        model = model.to("xpu")
                        model = ipex.optimize(
                            model,
                            dtype=dtype,
                            auto_kernel_selection=True,
                            weights_prepack=False
                        )
                        logger.info("IPEX XPU optimizations applied successfully")
                    except RuntimeError as e:
                        logger.warning(f"Error applying IPEX XPU optimizations: {e}, falling back to CPU")
                        device_type = "cpu"
                        model = model.to("cpu")
            
            # Store model in instance variables
            self._model = model
            self._device_type = device_type
            
        except Exception as e:
            logger.error(f"Error initializing IPEX model: {e}")
            raise
    
    def _get_text_embedding(self, text: str) -> List[float]:
        """Get embedding for a single text."""
        try:
            import torch
            
            # Check if model and tokenizer are initialized
            if self._tokenizer is None or self._model is None:
                raise ValueError("Tokenizer or model not initialized")
            
            # Tokenize input
            inputs = self._tokenizer([text], padding=True, truncation=True, return_tensors="pt", max_length=512)
            
            # Move inputs to the right device
            if self._device_type == "xpu":
                inputs = {k: v.to("xpu") for k, v in inputs.items()}
            
            # Generate embeddings
            with torch.no_grad():
                model_output = self._model(**inputs)
                
            # Apply mean pooling
            sentence_embeddings = self._mean_pooling(model_output, inputs['attention_mask'])
            
            # Normalize embeddings
            sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
            
            # Move back to CPU if needed
            if self._device_type == "xpu":
                sentence_embeddings = sentence_embeddings.cpu()
            
            # Convert to list
            return sentence_embeddings[0].tolist()
        except Exception as e:
            logger.error(f"Error getting text embedding: {e}")
            import numpy as np
            return np.random.randn(768).tolist()  # Fallback to random embedding
    
    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Get text embeddings for multiple texts."""
        return [self._get_text_embedding(text) for text in texts]
    
    def _get_query_embedding(self, query: str) -> List[float]:
        """Get query embedding."""
        return self._get_text_embedding(query)
    
    async def _aget_query_embedding(self, query: str) -> List[float]:
        """Async version of query embedding."""
        # For IPEX, we'll use the synchronous method in an executor
        import asyncio
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._get_text_embedding, query)
    
    async def _aget_text_embedding(self, text: str) -> List[float]:
        """Async version of text embedding."""
        # For IPEX, we'll use the synchronous method in an executor
        import asyncio
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._get_text_embedding, text)
    
    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Async version of text embeddings."""
        # For IPEX, we'll use the synchronous method in an executor
        import asyncio
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._get_text_embeddings, texts)
    
    def _mean_pooling(self, model_output, attention_mask):
        """Mean Pooling - Take attention mask into account for correct averaging."""
        import torch
        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
        return sum_embeddings / sum_mask

class SimpleVectorStore:
    """Simple vector store with multiple embedding options."""
    
    def __init__(
        self, 
        storage_dir: str = None,
        collection_name: str = "default",
        embedding_model: str = None,
        embedding_device: str = None
    ):
        """Initialize the vector store with settings derived from app config."""
        # Use app_config with supplied parameters as overrides
        self.storage_dir = storage_dir or getattr(app_config, "VECTOR_STORE_DIR", "./vector_store")
        self.collection_name = collection_name
        self.embedding_model = embedding_model or getattr(app_config, "EMBEDDING_MODEL", "local")
        self.embedding_device = embedding_device or getattr(app_config, "EMBEDDING_DEVICE", "auto")
        
        # Additional embedding-specific settings from app_config
        self.ipex_model = getattr(app_config, "IPEX_MODEL", "BAAI/bge-small-en-v1.5")
        self.ipex_fp16 = getattr(app_config, "IPEX_FP16", False)
        self.ollama_model = getattr(app_config, "OLLAMA_MODEL", "nomic-embed-text")
        self.ollama_url = getattr(app_config, "OLLAMA_URL", "http://localhost:11434")
        
        # Prompt generation settings
        self.generate_prompts = os.environ.get("GENERATE_PROMPTS", "").lower() in ("true", "1", "yes") or getattr(app_config, "GENERATE_PROMPTS", True)
        self.use_ollama_for_prompts = os.environ.get("USE_OLLAMA_FOR_PROMPTS", "").lower() in ("true", "1", "yes") or getattr(app_config, "USE_OLLAMA_FOR_PROMPTS", True)
        self.prompt_model = os.environ.get("PROMPT_MODEL") or getattr(app_config, "PROMPT_MODEL", "gemma:2b")
        self.multi_llm_prompts = os.environ.get("MULTI_LLM_PROMPTS", "").lower() in ("true", "1", "yes") or getattr(app_config, "MULTI_LLM_PROMPTS", False)
        
        logger.info(f"Prompt generation settings: enabled={self.generate_prompts}, use_ollama={self.use_ollama_for_prompts}, model={self.prompt_model}, multi_llm={self.multi_llm_prompts}")
        
        # Initialize prompt generator if prompt generation is enabled
        if self.generate_prompts:
            try:
                from vector_store_prompts import PromptGenerator
                logger.info(f"Initializing prompt generator with model: {self.prompt_model}")
                self.prompt_generator = PromptGenerator(
                    generate_prompts=self.generate_prompts,
                    use_ollama_for_prompts=self.use_ollama_for_prompts,
                    prompt_model=self.prompt_model,
                    ollama_url=self.ollama_url,
                    multi_llm_prompts=self.multi_llm_prompts
                )
            except ImportError as e:
                logger.warning(f"Could not import PromptGenerator: {e}, prompt generation will be disabled")
                self.prompt_generator = None
        else:
            logger.info("Prompt generation is disabled")
            self.prompt_generator = None
        
        self._index = None
        self._documents = []
        
        # Ensure storage directory exists
        os.makedirs(self.storage_dir, exist_ok=True)
        os.makedirs(os.path.join(self.storage_dir, collection_name), exist_ok=True)
        
        # Initialize the vector index
        self._initialize_index()

    def _create_document_fingerprint(self, document) -> str:
        """
        Create a unique fingerprint for document deduplication.
        
        Args:
            document: Document dictionary or string
            
        Returns:
            String fingerprint (SHA-256 hash)
        """
        # Handle both string and dictionary inputs
        if isinstance(document, str):
            # If a string is passed, use it directly as the text content
            text = document
            metadata = {}
        else:
            # Otherwise, extract text and metadata from the dictionary
            text = document.get('text', '')
            metadata = document.get('metadata', {})
        
        # Include important metadata fields in the fingerprint
        key_metadata = []
        for field in ['title', 'id', 'source', 'url', 'file_path']:
            if field in metadata and metadata[field]:
                key_metadata.append(f"{field}:{metadata[field]}")
        
        # Combine text and key metadata for fingerprinting
        fingerprint_content = text + '|' + '|'.join(key_metadata)
        
        # Create SHA-256 hash
        return hashlib.sha256(fingerprint_content.encode('utf-8')).hexdigest()

    def _initialize_index(self):
        """Initialize vector index or load an existing one."""
        try:
            from llama_index.core import (
                VectorStoreIndex, 
                StorageContext,
                load_index_from_storage
            )
            from llama_index.core.settings import Settings
            
            # Setup embedding model
            embed_model = None
            if self.embedding_model == "ipex":
                # Create Intel IPEX optimized embedding model
                embed_model = self._create_ipex_embedding_model()
            elif self.embedding_model == "ollama":
                # Create Ollama embedding model
                embed_model = self._create_ollama_embedding_model()
            else:
                # Default to local HuggingFace embedding - avoid asyncio.run()
                try:
                    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
                    # Handle different execution contexts properly
                    try:
                        # Check if we're in an event loop
                        loop = asyncio.get_event_loop()
                        if loop.is_running():
                            # We're already in an async context
                            def create_embedding():
                                return HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
                            # Run synchronously in a thread
                            import concurrent.futures
                            with concurrent.futures.ThreadPoolExecutor() as executor:
                                embed_model = executor.submit(create_embedding).result()
                        else:
                            # No running loop, create synchronously
                            embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
                    except RuntimeError:
                        # No event loop, create synchronously
                        logger.info("Creating HuggingFace embedding synchronously (no event loop)")
                        embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
                except ImportError:
                    logger.warning("HuggingFace embedding not available, using default")
                    embed_model = None
            
            # Set up Settings object with our embedding model
            if embed_model:
                # Update the global settings with our embed model
                Settings.embed_model = embed_model
                logger.info(f"Settings configured with custom embedding model: {type(embed_model).__name__}")
            
            # Check if index exists
            index_dir = os.path.join(self.storage_dir, self.collection_name)
            docstore_path = os.path.join(index_dir, "docstore.json")
            if os.path.exists(docstore_path):
                # Load existing index
                logger.info(f"Loading existing index from {index_dir}")
                storage_context = StorageContext.from_defaults(persist_dir=index_dir)
                # Load using the new approach (no ServiceContext)
                self._index = load_index_from_storage(storage_context)
                
                # Load document metadata
                metadata_path = os.path.join(index_dir, "document_metadata.json")
                if os.path.exists(metadata_path):
                    with open(metadata_path, 'r') as f:
                        self._documents = json.load(f)
                logger.info(f"Loaded index with {len(self._documents)} documents")
            else:
                # Create new index
                logger.info(f"Creating new index in {index_dir}")
                from llama_index.core.schema import Document
                # Create a simple document for initialization
                doc = Document(text="Placeholder document for initialization")
                # Create index using the new approach (no ServiceContext)
                self._index = VectorStoreIndex.from_documents([doc])
                # Save the index
                os.makedirs(index_dir, exist_ok=True)
                self._index.storage_context.persist(persist_dir=index_dir)
                # Initialize documents list
                self._documents = []
                # Save metadata synchronously to avoid asyncio issues
                metadata_path = os.path.join(index_dir, "document_metadata.json")
                with open(metadata_path, 'w') as f:
                    json.dump(self._documents, f)
                logger.info("Created new vector index")
        except Exception as e:
            logger.error(f"Error initializing vector index: {e}")
            self._index = None

    def _create_ipex_embedding_model(self):
        """
        Create an Intel IPEX optimized embedding model for LlamaIndex.
        
        Returns:
            IPEXEmbedding: Configured embedding model for Intel optimization
        """
        logger.info(f"Creating IPEX embedding model with model: {self.ipex_model}, device: {self.embedding_device}, fp16: {self.ipex_fp16}")
        # Check if the IPEXEmbedding class is defined in this module
        if 'IPEXEmbedding' in globals():
            embedding_class = globals()['IPEXEmbedding']
        else:
            # If not defined, we need to define it here
            from llama_index.core.embeddings import BaseEmbedding
            embedding_class = IPEXEmbedding
        
        try:
            # Create and return the IPEX embedding model
            return embedding_class(
                model_name=self.ipex_model,
                device=self.embedding_device,
                use_fp16=self.ipex_fp16
            )
        except Exception as e:
            logger.error(f"Failed to create IPEX embedding model: {e}")
            # Return a default embedding model as fallback
            from llama_index.embeddings.huggingface import HuggingFaceEmbedding
            logger.warning("Falling back to standard HuggingFace embedding")
            return HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

    async def _load_checkpoint(self, checkpoint_path: str) -> Dict[str, Any]:
        """
        Load checkpoint data to resume processing, using async file I/O.
        
        Args:
            checkpoint_path: Path to the checkpoint file
            
        Returns:
            Dictionary containing checkpoint data with completed fingerprints, pending nodes and docs
        """
        try:
            import aiofiles
            import pickle
            
            if not os.path.exists(checkpoint_path):
                logger.info(f"No checkpoint found at {checkpoint_path}, starting from scratch")
                return {'completed_fingerprints': [], 'pending_nodes': [], 'pending_docs': []}
            
            async with aiofiles.open(checkpoint_path, 'rb') as f:
                content = await f.read()
                data = pickle.loads(content)
            
            logger.info(f"Loaded checkpoint with {len(data.get('completed_fingerprints', []))} completed documents")
            return data
        except Exception as e:
            logger.error(f"Error loading checkpoint: {e}")
            return {'completed_fingerprints': [], 'pending_nodes': [], 'pending_docs': []}

    async def _save_checkpoint(self, checkpoint_path: str, data: Dict[str, Any]) -> bool:
        """
        Save checkpoint data to resume from interruptions, using async file I/O.
            
        Args:
            checkpoint_path: Path to save the checkpoint
            data: Dictionary with checkpoint data
            
        Returns:
            Success status
        """
        try:
            import aiofiles
            import pickle
            
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
            
            async with aiofiles.open(checkpoint_path, 'wb') as f:
                serialized_data = pickle.dumps(data)
                await f.write(serialized_data)
            
            return True
        except Exception as e:
            logger.error(f"Error saving checkpoint: {e}")
            return False

    async def _save_document_metadata(self, index_dir: str) -> bool:
        """
        Save document metadata to disk using async I/O.
        
        Args:
            index_dir: Directory where the vector store index is saved
            
        Returns:
            Success status
        """
        try:
            import aiofiles
            import json
            
            metadata_path = os.path.join(index_dir, "document_metadata.json")
            os.makedirs(os.path.dirname(metadata_path), exist_ok=True)
            
            # Diagnostic: Count prompts before saving
            prompt_count = 0
            multi_prompt_count = 0
            for doc in self._documents:
                metadata = doc.get('metadata', {})
                if 'prompt' in metadata and metadata['prompt']:
                    prompt_count += 1
                elif 'multi_llm_prompts' in metadata and metadata['multi_llm_prompts']:
                    multi_prompt_count += 1
            
            logger.info(f"About to save metadata: {len(self._documents)} documents, {prompt_count} with prompts, {multi_prompt_count} with multi-prompts")
            
            # Use a custom JSON serializer to handle any non-serializable objects
            def json_serializer(obj):
                if isinstance(obj, (dict, list, str, int, float, bool, type(None))):
                    return obj
                return str(obj)
            
            json_data = json.dumps(self._documents, default=json_serializer)
            
            async with aiofiles.open(metadata_path, 'w') as f:
                await f.write(json_data)
                
            logger.debug(f"Document metadata saved to {metadata_path}")
            return True
        except Exception as e:
            logger.error(f"Error saving document metadata: {e}")
            return False

    async def add_documents(self, documents: List[Any], show_progress: bool = True) -> bool:
        """ 
        Add documents to the vector store with deduplication, progress tracking, and resumption capability.
        
        Args:
            documents: List of documents (strings or dictionaries with 'text' field)
            show_progress: Whether to show a progress bar
            
        Returns:
            Success status
        """
        if not self._index:
            logger.warning("Vector store index not initialized")
            return False
        try:
            # Normalize document format
            normalized_documents = []
            for doc in documents:
                if isinstance(doc, str):
                    normalized_documents.append({"text": doc})
                elif isinstance(doc, dict) and 'text' in doc:
                    normalized_documents.append(doc)
                else:
                    logger.warning(f"Invalid document format, skipping: {type(doc)}")
            
            # Create a set of existing document fingerprints for deduplication
            existing_fingerprints = set()
            for doc in self._documents:
                fingerprint = doc.get('metadata', {}).get('fingerprint')
                if fingerprint:
                    existing_fingerprints.add(fingerprint)
            
            # First pass: identify new documents to process
            new_documents = []
            duplicates = 0
            for doc in normalized_documents:
                # Create fingerprint for deduplication
                fingerprint = self._create_document_fingerprint(doc)
                # Skip if this document already exists
                if fingerprint in existing_fingerprints:
                    duplicates += 1
                    continue
                # Add fingerprint to metadata and track for processing
                if 'metadata' not in doc:
                    doc['metadata'] = {}
                doc['metadata']['fingerprint'] = fingerprint
                new_documents.append(doc)
                # Add to existing fingerprints to avoid duplicates within this batch
                existing_fingerprints.add(fingerprint)
            
            logger.info(f"Found {len(new_documents)} new documents to process (skipped {duplicates} duplicates)")
            if not new_documents:
                logger.info(f"No new documents to add (skipped {duplicates} duplicates)")
                return True
            
            # Check for checkpoint file
            checkpoint_path = os.path.join(self.storage_dir, self.collection_name, "checkpoint.pkl")
            checkpoint_data = await self._load_checkpoint(checkpoint_path)
            
            # Extract previous progress from checkpoint for processing
            completed_fingerprints = set(checkpoint_data.get('completed_fingerprints', []))
            nodes_to_add = checkpoint_data.get('pending_nodes', [])
            docs_to_add = checkpoint_data.get('pending_docs', [])
            
            # Filter out already processed documents
            docs_to_process = []
            for doc in new_documents:
                fingerprint = doc['metadata']['fingerprint']
                if fingerprint not in completed_fingerprints:
                    docs_to_process.append(doc)
            logger.info(f"Processing {len(docs_to_process)} documents (resuming with {len(nodes_to_add)} nodes from checkpoint)")
            
            from llama_index.core.schema import TextNode
            # Process documents with progress bar
            with Progress(
                SpinnerColumn(),
                TextColumn("[bold blue]{task.description}"),
                BarColumn(),
                TextColumn("[bold]{task.completed}/{task.total}"),
                TimeElapsedColumn(),
                TimeRemainingColumn()
            ) as progress:
                task = progress.add_task(
                    "[green]Adding documents",
                    total=len(docs_to_process),
                    completed=0
                )
                for i, doc in enumerate(docs_to_process):
                    try:
                        # Create a unique ID
                        doc_id = f"{doc.get('metadata', {}).get('id', f'doc_{i}')}_{len(self._documents) + len(docs_to_add)}"
                        # Create a node
                        node = TextNode(
                            text=doc['text'],
                            metadata=doc.get('metadata', {}),
                            id_=doc_id
                        )
                        
                        # Add to document tracking
                        nodes_to_add.append(node)
                        doc_entry = {
                            'id': doc_id,
                            'metadata': doc.get('metadata', {}),
                            'timestamp': datetime.now().isoformat()
                        }
                        docs_to_add.append(doc_entry)
                        completed_fingerprints.add(doc['metadata']['fingerprint'])
                        
                        # Debugging logs for prompt generation
                        logger.debug(f"Prompt generation check - generate_prompts: {self.generate_prompts}, has prompt: {'prompt' in doc.get('metadata', {})}")
                        logger.debug(f"Prompt generator available: {hasattr(self, 'prompt_generator')}")
                        if hasattr(self, 'prompt_generator'):
                            logger.debug(f"Prompt generator type: {type(self.prompt_generator)}")
                        
                        # Generate enhanced prompt if needed
                        if self.generate_prompts and 'prompt' not in doc.get('metadata', {}):
                            try:
                                if hasattr(self, 'prompt_generator'):
                                    logger.debug(f"Generating prompt for document {i+1}/{len(docs_to_process)}")
                                    if self.multi_llm_prompts:
                                        multi_prompts = await self.prompt_generator.generate_multi_llm_prompts(doc)
                                        
                                        # Ensure any custom objects are converted to strings for serialization
                                        sanitized_prompts = {}
                                        for k, v in multi_prompts.items():
                                            if isinstance(v, str):
                                                sanitized_prompts[k] = v
                                            elif isinstance(v, list):
                                                sanitized_prompts[k] = [str(item) if not isinstance(item, str) else item for item in v]
                                            else:
                                                sanitized_prompts[k] = str(v)
                                        
                                        # Update all related objects with the prompt data
                                        doc['metadata']['multi_llm_prompts'] = sanitized_prompts
                                        node.metadata['multi_llm_prompts'] = sanitized_prompts
                                        doc_entry['metadata']['multi_llm_prompts'] = sanitized_prompts
                                        
                                        logger.info(f"Added multi-LLM prompts to document {doc_id} ({len(sanitized_prompts)} prompt types)")
                                    else:
                                        prompt = await self.prompt_generator.generate_prompt(doc)
                                        if prompt:
                                            # Update all related objects with the prompt data
                                            doc['metadata']['prompt'] = prompt
                                            node.metadata['prompt'] = prompt
                                            doc_entry['metadata']['prompt'] = prompt
                                            logger.info(f"Added prompt to document {doc_id} [{len(prompt)} chars]")
                                        else:
                                            logger.warning(f"Generated empty prompt for document {doc_id}")
                            except Exception as e:
                                logger.error(f"Error generating prompt for document {i}: {e}")
                        
                        # Save checkpoint every 10 documents
                        if (i + 1) % 10 == 0:
                            await self._save_checkpoint(checkpoint_path, {
                                'completed_fingerprints': list(completed_fingerprints),
                                'pending_nodes': nodes_to_add,
                                'pending_docs': docs_to_add
                            })
                            progress.update(task, description=f"[cyan]Checkpoint saved ({i+1}/{len(docs_to_process)})")
                        
                        # Update progress
                        progress.update(task, advance=1, description=f"[green]Processing document {i+1}/{len(docs_to_process)}")
                    except Exception as e:
                        logger.error(f"Error processing document {i}: {e}")
                        continue
            
            # Final checkpoint before index insertion
            await self._save_checkpoint(checkpoint_path, {
                'completed_fingerprints': list(completed_fingerprints),
                'pending_nodes': nodes_to_add,
                'pending_docs': docs_to_add
            })
            
            # Add nodes to index
            if nodes_to_add:
                logger.info(f"Inserting {len(nodes_to_add)} nodes into vector index...")
                self._index.insert_nodes(nodes_to_add)
            
                # Update document list
                self._documents.extend(docs_to_add)
                
                # Save index and metadata
                index_dir = os.path.join(self.storage_dir, self.collection_name)
                logger.info("Saving index to disk...")
                self._index.storage_context.persist(persist_dir=index_dir)
                await self._save_document_metadata(index_dir)
                
                # Remove checkpoint after successful completion
                if os.path.exists(checkpoint_path):
                    os.remove(checkpoint_path)
                    logger.info("Checkpoint cleared after successful completion")
            
            logger.info(f"Added {len(nodes_to_add)} documents to vector store (skipped {duplicates} duplicates)")
            return True
        except Exception as e:
            logger.error(f"Error adding documents to vector store: {e}")
            return False